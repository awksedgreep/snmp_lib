defmodule SnmpLib.MIB.Lexer do
  @moduledoc """
  Direct port of Erlang snmpc_tok.erl tokenizer to Elixir.
  
  This is a 1:1 port of the official Erlang SNMP compiler tokenizer
  from OTP lib/snmp/src/compile/snmpc_tok.erl
  
  Original copyright: Ericsson AB 1996-2025 (Apache License 2.0)
  """

  # Token structure: {Category, Line, Value} | {Category, Line}
  # Category == keyword => 2-tuple (otherwise 3-tuple)
  # Category: :integer | :quote | :string | :variable | :atom | keyword | single_char

  @reserved_words MapSet.new([
    "ACCESS", "AGENT-CAPABILITIES", "APPLICATION", "AUGMENTS", "BEGIN",
    "BITS", "CHOICE", "CONTACT-INFO", "COUNTER", "COUNTER32", "COUNTER64", 
    "DEFINITIONS", "DESCRIPTION", "DISPLAY-HINT", "END", "ENTERPRISE",
    "EXPORTS", "FROM", "GAUGE", "GAUGE32", "GROUP", "IDENTIFIER",
    "IMPLICIT", "IMPORTS", "INDEX", "INTEGER", "INTEGER32", "IpAddress",
    "LAST-UPDATED", "MAX-ACCESS", "MIN-ACCESS", "MODULE", "MODULE-COMPLIANCE",
    "MODULE-IDENTITY", "NOTIFICATION-GROUP", "NOTIFICATION-TYPE", "OBJECT",
    "OBJECT-GROUP", "OBJECT-IDENTITY", "OBJECT-TYPE", "OBJECTS", "OCTET",
    "OF", "ORGANIZATION", "REFERENCE", "REVISION", "SEQUENCE", "SIZE",
    "STATUS", "STRING", "SYNTAX", "TEXTUAL-CONVENTION", "TimeTicks",
    "TRAP-TYPE", "UNITS", "UNIVERSAL", "Unsigned32", "VARIABLES", "WRITE-SYNTAX"
  ])

  @doc """
  Tokenize a string into SNMP MIB tokens.
  Returns {:ok, tokens} or {:error, reason}
  """
  def tokenize(input, opts \\ []) do
    try do
      tokens = do_tokenize(to_charlist(input), 1, [])
      {:ok, Enum.reverse(tokens)}
    rescue
      e -> {:error, Exception.message(e)}
    catch
      {:error, reason} -> {:error, reason}
    end
  end

  # Main tokenization loop
  defp do_tokenize([], _line, acc), do: acc

  # Skip whitespace
  defp do_tokenize([?\s | rest], line, acc), do: do_tokenize(rest, line, acc)
  defp do_tokenize([?\t | rest], line, acc), do: do_tokenize(rest, line, acc)
  defp do_tokenize([?\r | rest], line, acc), do: do_tokenize(rest, line, acc)

  # Handle newlines
  defp do_tokenize([?\n | rest], line, acc), do: do_tokenize(rest, line + 1, acc)

  # Skip comments (-- to end of line)
  defp do_tokenize([?-, ?- | rest], line, acc) do
    rest_after_comment = skip_comment(rest)
    do_tokenize(rest_after_comment, line, acc)
  end

  # Handle string literals
  defp do_tokenize([?" | rest], line, acc) do
    {string_value, rest_after_string} = collect_string(rest, [])
    token = {:string, line, List.to_string(string_value)}
    do_tokenize(rest_after_string, line, [token | acc])
  end

  # Handle quoted literals
  defp do_tokenize([?' | rest], line, acc) do
    {quote_value, rest_after_quote} = collect_quote(rest, [])
    token = {:quote, line, List.to_string(quote_value)}
    do_tokenize(rest_after_quote, line, [token | acc])
  end

  # Handle integers
  defp do_tokenize([ch | _] = input, line, acc) when ch >= ?0 and ch <= ?9 do
    {int_value, rest} = get_integer(input, 0)
    token = {:integer, line, int_value}
    do_tokenize(rest, line, [token | acc])
  end

  # Handle negative integers
  defp do_tokenize([?- | [ch | _] = rest], line, acc) when ch >= ?0 and ch <= ?9 do
    {int_value, rest_after_int} = get_integer(rest, 0)
    token = {:integer, line, -int_value}
    do_tokenize(rest_after_int, line, [token | acc])
  end

  # Handle identifiers and keywords
  defp do_tokenize([ch | _] = input, line, acc) when (ch >= ?a and ch <= ?z) or (ch >= ?A and ch <= ?Z) do
    {name, rest} = get_name(input, [])
    token = make_name_token(name, line)
    do_tokenize(rest, line, [token | acc])
  end

  # Handle single character tokens
  defp do_tokenize([ch | rest], line, acc) do
    token = {List.to_atom([ch]), line}
    do_tokenize(rest, line, [token | acc])
  end

  # Skip comments until end of line
  defp skip_comment([?\n | rest]), do: [?\n | rest]
  defp skip_comment([_ | rest]), do: skip_comment(rest)
  defp skip_comment([]), do: []

  # Collect string characters until closing quote
  defp collect_string([?" | rest], acc), do: {Enum.reverse(acc), rest}
  defp collect_string([?\\ | [ch | rest]], acc), do: collect_string(rest, [ch | acc])
  defp collect_string([ch | rest], acc), do: collect_string(rest, [ch | acc])
  defp collect_string([], _acc), do: throw({:error, "Unterminated string"})

  # Collect quote characters until closing quote
  defp collect_quote([?' | rest], acc), do: {Enum.reverse(acc), rest}
  defp collect_quote([ch | rest], acc), do: collect_quote(rest, [ch | acc])
  defp collect_quote([], _acc), do: throw({:error, "Unterminated quote"})

  # Parse integer value
  defp get_integer([ch | rest], acc) when ch >= ?0 and ch <= ?9 do
    get_integer(rest, acc * 10 + (ch - ?0))
  end
  defp get_integer(rest, acc), do: {acc, rest}

  # Collect name characters (letters, digits, hyphens)
  defp get_name([ch | rest], acc) when (ch >= ?a and ch <= ?z) or 
                                      (ch >= ?A and ch <= ?Z) or
                                      (ch >= ?0 and ch <= ?9) or
                                      ch == ?- do
    get_name(rest, [ch | acc])
  end
  defp get_name(rest, acc), do: {Enum.reverse(acc), rest}

  # Create appropriate token for name
  defp make_name_token(name, line) do
    name_string = List.to_string(name)
    
    cond do
      MapSet.member?(@reserved_words, name_string) ->
        # Convert reserved word to keyword atom (lowercase with underscores)
        keyword_atom = name_string 
                      |> String.replace("-", "_")
                      |> String.downcase()
                      |> String.to_atom()
        {keyword_atom, line}
      hd(name) >= ?A and hd(name) <= ?Z ->
        # Variable (starts with uppercase)
        {:variable, line, name_string}
      true ->
        # Atom (starts with lowercase)
        {:atom, line, name_string}
    end
  end

  @doc """
  Format error messages
  """
  def format_error(reason), do: to_string(reason)

  @doc """
  Test function
  """
  def test do
    test_input = """
    TestMib DEFINITIONS ::= BEGIN
      testObject OBJECT-TYPE
        SYNTAX INTEGER { active(1), inactive(2) }
        MAX-ACCESS read-only
        STATUS current
        DESCRIPTION "Test object"
        ::= { test 1 }
    END
    """
    
    case tokenize(test_input) do
      {:ok, tokens} ->
        IO.puts("Tokenization successful!")
        Enum.each(tokens, fn token -> IO.inspect(token) end)
      {:error, reason} ->
        IO.puts("Tokenization failed: #{reason}")
    end
  end
end